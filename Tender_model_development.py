# -*- coding: utf-8 -*-
"""smart_proposal_system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i-Tvj1xg1pBgXgOFPxsc_gzL_1y5yC4-
"""

!pip install transformers datasets torch

from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Use the GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Load the pre-trained model from "CoffeeAddict93/gpt2-medium-modest-proposal"
model_name = "CoffeeAddict93/gpt2-medium-modest-proposal"
model = GPT2LMHeadModel.from_pretrained(model_name)

import pandas as pd
from datasets import Dataset

# Load your dataset
df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/data.csv')

# Prepare the dataset for Hugging Face
df['text'] = df.apply(lambda row: (
    f"RFP details: {row['RFP details']}\n"
    f"Organization Name: {row['Organization Name']}\n"
    f"Address: {row['Address']}\n"
    f"Program description: {row['Program description']}\n"
    f"Introduction: {row['Introduction']}\n"
    f"Executive Summary: {row['Executive Summary']}\n"
    f"Methods & Approach: {row['Methods & Approach']}\n"
    f"Additional comments (optional): {row['Additional comments (optional)']}\n"
    f"Flag: {row['flag']}\n"
    f"Username: {row['username']}\n"
), axis=1)

# Convert to Hugging Face dataset
hf_dataset = Dataset.from_pandas(df[['text']])

from sklearn.model_selection import train_test_split

# Split the dataset into training and validation sets
train_df, val_df = train_test_split(df, test_size=0.1)  # 10% for validation

# Convert to Hugging Face datasets
train_dataset = Dataset.from_pandas(train_df[['text']])
val_dataset = Dataset.from_pandas(val_df[['text']])

# Assign eos_token as the padding token
tokenizer.pad_token = tokenizer.eos_token

# Tokenize the dataset
def tokenize_function(examples):
    encodings = tokenizer(examples['text'], padding="max_length", truncation=True, max_length=512)
    encodings['labels'] = encodings['input_ids']
    return encodings

tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/Colab_Notebooks/proposal_gpt_model_result",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,  # Add validation dataset
)

# Fine-tune the model
trainer.train()

model.save_pretrained("/content/drive/MyDrive/Colab_Notebooks/proposal_gpt_model_result/fine-tuned-gpt2-medium-modest-proposal")
tokenizer.save_pretrained("/content/drive/MyDrive/Colab_Notebooks/proposal_gpt_model_result/fine-tuned-gpt2-medium-modest-proposal")

# Load the fine-tuned model
fine_tuned_model = GPT2LMHeadModel.from_pretrained("/content/drive/MyDrive/Colab_Notebooks/proposal_gpt_model_result/fine-tuned-gpt2-medium-modest-proposal")
fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained("/content/drive/MyDrive/Colab_Notebooks/proposal_gpt_model_result/fine-tuned-gpt2-medium-modest-proposal")

# Generate text
prompt = "What is the address of Alberta Innovates organization?"
input_ids = fine_tuned_tokenizer.encode(prompt, return_tensors='pt')
output = fine_tuned_model.generate(input_ids, max_length=150, num_return_sequences=1)

# Decode and print the result
print(fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True))
